\documentclass{article}
\usepackage{amsmath,amsthm,amssymb,graphicx,subfig,mathrsfs}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\begin{document}
\title{Random trigonometric polynomials}
\author{Jordan Bell}
\date{April 23, 2016}
\maketitle


Borwein and Lockhart \cite{MR1814174}. 

\section{$L^2$ norm}
Let $\mathbb{T}=\mathbb{R}/\mathbb{Z}$, and for $f:\mathbb{T} \to \mathbb{C}$ let
\[
\norm{f}_{L^p}^p =  \int_0^1 |f(\theta)|^p d\theta.
\]
Let $X_1,X_2,\ldots:(\Omega,\mathscr{F},P) \to (\mathbb{R},\mathscr{B}_{\mathbb{R}})$ be 
independent identically distributed random variables with mean $0$ and variance $1$, and define
\[
q_n(\theta) = \sum_{j=0}^{n-1} X_j e^{2\pi ij\theta},\qquad \theta \in \mathbb{T}.
\]
By Plancherel's theorem,
\[
\norm{q_n}_{L^2}^2 = \sum_{j=0}^{n-1} X_j^2.
\]

Let $Y_j=X_j^2-1$, which are independent and identically distributed.
Then
\[
\norm{q_n}_{L^2}^2-n = \sum_{j=0}^{n-1} Y_j.
\]
We have
\[
E(Y_j) = E(X_j^2)-1 = 0.
\]
Write
\[
\sigma^2 = E(Y_j^2) = E(X_j^4-2X_j^2+1) = E(X_j^4) - 2E(X_j^2) + 1 = E(X_j^4) - 1,
\]
and let
\[
Z_n =
\frac{\sum_{j=0}^{n-1} Y_j}{\sigma \sqrt{n}}
= \frac{\norm{q_n}_{L^2}^2-n}{\sigma \sqrt{n}},
\]
which has mean $0$ and variance $1$. 
Because $Y_1,Y_2,\ldots$ are independent and identically distributed with mean $0$ and variance $\sigma^2$,
by the central limit theorem, $Z_n \to \gamma_1$ in distribution, where $\gamma_{t^2}$ is the Gaussian measure
on $\mathbb{R}$ with variance $t^2$. 

\begin{theorem}
\[
E(\norm{q_n}_{L^2}) = \sqrt{n} - \frac{1}{8} \frac{\sigma^2}{\sqrt{n}} + O(n^{-1}),
\]
where $\sigma^2 = E(X_j^4) - 1$.
\end{theorem}
\begin{proof}
Because 
\[
\norm{q_n}_{L^2} = \sqrt{n+\sigma \sqrt{n} Z_n},
\]
we have
\[
\norm{q_n}_{L^2} - \sqrt{n} = \sqrt{n} \left(\sqrt{ 1+\frac{\sigma Z_n}{\sqrt{n}}} - 1 \right).
\]
Using the binomial series,
\[
\sqrt{ 1+\frac{\sigma Z_n}{\sqrt{n}}} =
1+\frac{\sigma Z_n}{2\sqrt{n}} - \frac{1}{8} \frac{\sigma^2 Z_n^2}{n} + O\left(\frac{Z_n^3}{n^{3/2}}\right),
\]
so 
\begin{equation}
\norm{q_n}_{L^2} - \sqrt{n}  = \frac{\sigma Z_n}{2} - \frac{1}{8} \frac{\sigma^2 Z_n^2}{\sqrt{n}} + O \left( \frac{Z_n^3}{n}\right).
\label{binomial}
\end{equation}

We expand $Z_n^4$: it is
\[
\sigma^{-4} n^{-2} \left( \sum_j Y_j^4 + \sum_{j \neq k} Y_j^3 Y_k + \sum_{j \neq k} Y_j^2 Y_k^2
+ \sum_{j \neq k \neq p} Y_k^2 Y_j Y_p
+\sum_{j \neq k \neq p \neq q} Y_jY_kY_pY_q\right). 
\]
Then, because $E(Y_j)=0$ and $E(Y_j^2)=\sigma^2$, we get
\[
E(Z_n^4) = \sigma^{-4} n^{-2} ( nE(Y_1^4) + n(n-1)\sigma^4 ).
\]
Now define
\[
\tau = E(Y_j^4),
\]
so
\[
E(Z_n^4) = \sigma^{-4} n^{-2}(n\tau + n(n-1)\sigma^4)
=1+\frac{1}{n} \left( \frac{\tau}{\sigma^4} -1\right).
\]
But $E(|Z_n|^3)^{1/3} \leq E(|Z_n|^4)^{1/4}$, so
\[
E(|Z_n|^3) \leq \left(1+\frac{1}{n}\left(\frac{\tau}{\sigma^4}-1\right)\right)^{3/4} \leq 1.
\]
Taking the expectation of \eqref{binomial}, because $E(Z_n)=0$ and $E(Z_n^2)=1$,
\[
E(\norm{q_n}_{L^2}) = \sqrt{n} - \frac{1}{8} \frac{\sigma^2}{\sqrt{n}} + O(n^{-1}).
\]
\end{proof}


\section{Berry-Esseen}
\begin{theorem}
\[
\norm{q_n}_{L^2} - \sqrt{n} \to \frac{\sigma}{2} Z
\]
in distribution.
\end{theorem}
\begin{proof}
Write
\[
\rho = E(|Y_j|^3)
\]
and 
\[
S_n = \sum_{j=0}^{n-1} Y_j,
\]
and let
\[
F_n(x) = P(S_n \leq \sigma n^{1/2} x)
\]
and 
\[
\Phi(x) = P(Z \leq x).
\]
The Berry-Esseen theorem \cite[p.~262, Theorem 5.6.1]{pinsky} states that there is some $C$, not depending on the random variables
$Y_j$, such that for all $n$ and for all $x \in \mathbb{R}$,
\[
|F_n(x) - \Phi(x)| \leq \frac{C\rho}{\sigma^3 \sqrt{n}}.
\]

Now,
\[
Z_n = \frac{1}{\sigma n^{1/2}} \sum_{j=0}^{n-1} Y_j = \frac{S_n}{\sigma n^{1/2}},
\]
so
\[
F_n(x) = P\left( \frac{S_n}{\sigma n^{1/2}} \leq x\right) = P(Z_n \leq x).
\]
For $A>0$,
\[
P(|Z_n| \geq A) = P(Z_n \geq A) + P(Z_n \leq -A)
=1-P(Z_n<A)+P(Z_n \leq -A).
\]

\[
P(|Z_n| \geq A) - P(|Z| \geq A) = P(Z<A) - P(Z_n<A) + P(Z_n \leq _A) - P(Z \leq -A).
\]
Then
\[
|P(|Z_n| \geq A) - P(|Z| \geq A)|
\leq |\Phi(A)-F_n(A)| +P(Z_n = A) + |F_n(-A) - \Phi(-A)|,
\]
so by the Berry-Esseen theorem,
\[
|P(|Z_n| \geq A) - P(|Z| \geq A)| \leq P(Z_n = A) + 2\frac{C\rho}{\sigma^3 n^{1/2}}.
\]

Markov's inequality tells us
\[
P(|Z| \geq A) \leq \sqrt{\frac{2}{\pi}} \frac{e^{-A^2/2}}{A}.
\]
For $\epsilon>0$ and for $A=n^{1/4} e^{1/2}$,
\[
P(|Z| \geq A) = P(|Z|^2 \geq n^{1/2} \epsilon)
\leq \frac{\sqrt{\frac{2}{\pi}} \exp\left(-\frac{n^{1/2} \epsilon}{2} \right)}{n^{1/4} \epsilon^{1/2}}.
\]

Therefore $\frac{Z_n^2}{n^{1/2}} \to 0$ in probability and $\frac{|Z_n|^3}{n} \to 0$ in probability,
and because $\frac{\sigma Z_n}{2} \to \frac{\sigma}{2}Z$ in distribution, it follows that
\[
\norm{q_n}_{L^2} - \sqrt{n} \to \frac{\sigma}{2} Z
\]
in distribution.
\end{proof}




\section{$L^4$ norm}
\begin{theorem}
\[
E(\norm{q_n}_{L^4}^4) = 2n^2+ n(E(X_j^4)-2).
\]
\end{theorem}
\begin{proof}
\[
\norm{q_n}_{L^4}^4 = \int_0^{1} q_n(\theta)^2 \overline{q_n(\theta)^2} d\theta.
\]

Write $e(\theta)=e^{2\pi i\theta}$. 

\[
q_n^2 = \sum_j X_j^2 e(2j \theta) + \sum_{j \neq k} X_j X_k e(j\theta+k\theta).
\]

\begin{align*}
q_n^2 \overline{q_n^2}&=\sum_{j,p} X_j^2 e(2k\theta) X_p^2 e(-2p\theta)+\sum_p X_p^2 e(-2p\theta)
\sum_{j \neq k} X_j X_k e(k\theta+j\theta)\\
&+\sum_{p \neq q} X_p X_q e(-p\theta - q\theta) \sum_j X_j^2 e(2j\theta)\\
&+\sum_{p \neq q} X_p X_q e(-p\theta - q\theta) \sum_{j \neq k} X_j X_k e(k\theta+j\theta).
\end{align*}

Then
\begin{align*}
\int_0^1 q_n^2 \overline{q_n^2} d\theta&= \sum_j \sum_p X_j^2 X_p^2 \delta_{j-p,0}\\
&+\sum_p \sum_{j \neq k} X_p^2 X_j X_k \delta_{j+k-2p,0}\\
&+ \sum_j \sum_{p \neq q} X_j^2 X_p X_q \delta_{2j-p-q,0}\\
&+\sum_{j \neq k} \sum_{p \neq q} X_j X_k X_p X_q \delta_{j+k-p-q,0}.
\end{align*}

That is
\begin{align*}
\norm{q_n}_{L^4}^4&=\sum_j \sum_p X_j^2 X_p^2 \delta_{j,p}\\
&+\sum_p \sum_{j \neq k} X_p^2 X_j X_k \delta_{j+k,2p}\\
&+\sum_j \sum_{p \neq q} X_j^2 X_p X_q \delta_{p+q,2j}\\
&+\sum_{j \neq k} \sum_{p \neq q} X_j X_k X_p X_q \delta_{j+k,p+q}\\
&=\sum_j X_j^4 + 2 \sum_p \sum_{j \neq p} \sum_{k \neq p, k \neq j} X_p^2 X_j X_k \delta_{j+k,2p}\\
&+\sum_j \sum_{k \neq j} \sum_{p \neq j, p \neq k} \sum_{q \neq p, q \neq j, q \neq k} X_j X_k X_p X_q \delta_{j+k,p+q}\\
&+\sum_j \sum_{k \neq j} X_j^2 X_k^2 + \sum_j \sum_{k \neq j} X_j^2 X_k^2.
\end{align*}


Then
\begin{align*}
E(\norm{q_n}_{L^4}^4) &= \sum_j E(X_j^4) + 2\sum_j \sum_{k \neq j} E(X_j^2) E(X_k^2)\\
&=nE(X_j^4) + 2n^2 -2n.
\end{align*}
\end{proof}



\section{Gaussian random variables}
Suppose that the distribution of each $X_j$ is the standard Gaussian measure on $\mathbb{R}$, and write
\[
S_{n,\theta} = \sum_{j=0}^{n-1} X_j e^{2\pi ij\theta} = \sum_{j=0}^{n-1} X_j \cos 2\pi j \theta + i \sum_{j=0}^{n-1} X_j \sin 2\pi j\theta,
\quad n \geq 1, \quad \theta \in \mathbb{T}.
\]
Then for each $\theta \in \mathbb{T}$, there are $Z_\theta$ and $W_\theta$, each random variables with the standard Gaussian distribution,
such that
\[
S_{n,\theta} = (n/2)^{1/2} Z_\theta + i (n/2)^{1/2} W_\theta.
\]
Now, $|Z_\theta+iW_\theta|$ has density $t \mapsto t e^{-t^2/2}$, and then
\[
E(|Z_\theta+iW_\theta|^p) = 2^{p/2} \Gamma\left(1+\frac{p}{2}\right).
\]
Then
\[
E(|S_{n,\theta}|^p) = \left( \frac{n}{2} \right)^{p/2} 2^{p/2} \Gamma\left(1+\frac{p}{2}\right)
=n^{p/2} \Gamma\left(1+\frac{p}{2}\right).
\]


\bibliographystyle{plain}
\bibliography{randomtrig}

\end{document}